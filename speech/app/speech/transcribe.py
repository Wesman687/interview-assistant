import asyncio
import collections
import os
import queue
import numpy as np
import pyaudio
import threading
from google.cloud import speech
import webrtcvad
import whisper
from app.interview.ai_processing import process_ai_response
from app.utils.websocket_manager import websocket_manager
from app.speech.stream_config import streaming_config
from app.routes.speech import USE_WHISPER

# âœ… Google Cloud Speech-to-Text
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "service-account.json"
whisper_model = whisper.load_model("large", device="cuda")  # âœ… Forces GPU usage
vad = webrtcvad.Vad(3)

# âœ… Audio Handling
RATE = 16000
CHUNK = int(RATE / 10)
audio_queue = queue.Queue()
buffered_transcript = ""
lock = threading.Lock()
running = False  # âœ… Track if recognition is running
connected_clients = set()  # âœ… WebSocket Clients
running = False  # âœ… Track if recognition is running
idle = False  # âœ… Track if we are in Idle Mode
processing_state = False


# âœ… VAD Parameters
CHUNK_DURATION_MS = 30  
CHUNK_SIZE = int(RATE * CHUNK_DURATION_MS / 1000)  # Convert ms to samples
NUM_PADDING_CHUNKS = int(1500 / CHUNK_DURATION_MS)  # 1.5 seconds of silence detection


# âœ… Detect Silence Timer (10s threshold)
silence_timer = None

def is_silent(audio_chunk, threshold=500):
    """Check if an audio chunk is silent."""
    audio_data = np.frombuffer(audio_chunk, dtype=np.int16)
    return np.max(np.abs(audio_data)) < threshold  # âœ… Ignore low-volume sounds

def audio_callback(in_data, frame_count, time_info, status):
    """Capture audio from the microphone and wake up from idle if needed."""
    global idle, running
    if not running:
        return None, pyaudio.paAbort

    if idle:  
        print("ğŸ¤ Voice detected! Exiting Idle Mode and restarting transcription...")
        idle = False  
        asyncio.ensure_future(start_transcription()) 

    if not is_silent(in_data):  # âœ… Only queue non-silent audio
        audio_queue.put(in_data)

    return None, pyaudio.paContinue

def audio_generator():
    """Continuously fetch audio from the queue and send it to Google API."""
    while running:
        try:
            data = audio_queue.get(timeout=2)  # âœ… Use timeout to avoid deadlock
            if data is None:
                print("â³ Audio queue empty, entering idle mode...")
                continue  # âœ… Instead of stopping, keep waiting
            print(f"ğŸ¤ Streaming {len(data)} bytes of audio")
            yield speech.StreamingRecognizeRequest(audio_content=data)
        except queue.Empty:
            print("â³ Audio queue empty, stopping generator...")
            break  # âœ… Prevents iterating on an empty queue
        
async def transcribe_whisper():
    """Transcribes audio using Whisper AI with VAD to detect sentence boundaries."""
    print("ğŸ™ï¸ Using Whisper AI with VAD...")
    global buffered_transcript

    ring_buffer = collections.deque(maxlen=NUM_PADDING_CHUNKS)
    triggered = False
    voiced_frames = []
    
    while running:
        try:
            audio_chunk = audio_queue.get(timeout=2)

            # âœ… Speech Start Detection
            if not triggered:
                ring_buffer.append(audio_chunk)
                num_voiced = sum(vad.is_speech(frame, RATE) for frame in ring_buffer)
                if num_voiced > 0.8 * len(ring_buffer):  # If 80% of recent frames contain speech
                    print("ğŸ¤ Speech detected! Starting transcription...")
                    await websocket_manager.broadcast_status("speaking")
                    triggered = True
                    voiced_frames.extend(ring_buffer)
                    ring_buffer.clear()

            # âœ… Speech End Detection
            else:
                voiced_frames.append(audio_chunk)
                ring_buffer.append(audio_chunk)
                num_unvoiced = len(ring_buffer) - sum(vad.is_speech(frame, RATE) for frame in ring_buffer)

                if num_unvoiced > 0.90 * len(ring_buffer):  # If 90% of recent frames are silent
                    print("ğŸ”• Silence detected. Ending transcription...")

                    # âœ… Convert collected voiced frames into a single numpy array
                    final_audio = np.concatenate([np.frombuffer(frame, dtype=np.int16) for frame in voiced_frames])
                    final_audio = final_audio.astype(np.float32) / 32768.0  # Normalize audio for Whisper
                    processing_state = True
                    # âœ… Transcribe with Whisper (Pass raw audio data)
                    result = whisper_model.transcribe(final_audio, fp16=True)  
                    transcript = result["text"].strip()

                    if transcript:
                        print(f"ğŸ“ Whisper: {transcript}")
                        buffered_transcript += " " + transcript.strip()

                        transcription_payload = {"transcription": buffered_transcript.strip()}
                        await processing_transcription(transcription_payload)

                        buffered_transcript = ""  # âœ… Reset transcript

                    # âœ… Reset VAD state
                    triggered = False
                    voiced_frames = []
                    ring_buffer.clear()

        except queue.Empty:
            break
        except Exception as e:
            print(f"âŒ Whisper Transcription Error: {e}")

async def transcribe_google():
    global client
    global buffered_transcript, silence_timer
    print("ğŸ™ï¸ Listening for speech...")
    while running:
        responses = await asyncio.to_thread(client.streaming_recognize, streaming_config, audio_generator())

        try:
            for response in responses:
                for result in response.results:
                    transcript = result.alternatives[0].transcript.strip()
                    speaker = result.speaker_tag if hasattr(result, "speaker_tag") else None
                    speaker_label = "Recruiter" if speaker == 1 else "Me"

                    if not transcript:
                        continue  # âœ… Skip empty results
                    await websocket_manager.broadcast_status("speaking")
                    if result.is_final:  # âœ… Only send complete sentences
                        print(f"ğŸ“ Finalized: {transcript}")
                        # âœ… Store full sentence
                        buffered_transcript += " " + transcript.strip()  # âœ… Append instead of overwriting
                        processing_state = True
                        formatted_transcript = f"{speaker_label}: {transcript}"

                        print(f"ğŸ™ï¸ {formatted_transcript}")  # âœ… Debugging output

                        # âœ… Send filtered transcript to WebSocket
                        transcription_payload = {
                            "transcription": formatted_transcript,
                        }
                        await processing_transcription(transcription_payload)
                        # âœ… Reset transcript after sending
                        buffered_transcript = ""

                        # âœ… Refresh Google SpeechClient after every finalized sentence
                        print("ğŸ”„ Refreshing Google SpeechClient to prevent lag...")
                        client = speech.SpeechClient()  # âœ… Throw away the old client and start fresh

                    else:
                        print(f"ğŸ“ Interim: {transcript}")  # âœ… Debugging interim results

                    if silence_timer:
                        silence_timer.cancel()
                    silence_timer = asyncio.create_task(notify_silence())

        except queue.Empty:
            print("ğŸ”• No more audio in queue, stopping generator...")
        except Exception as e:
            print(f"âŒ Error in speech recognition: {e}")
            
async def processing_transcription(transcription_payload):
    """Process the transcription and send it to the AI model."""
    try:        
        await websocket_manager.broadcast_message(transcription_payload)
        print(f"ğŸ“¡ Sent Immediate Transcription: {transcription_payload}")                    
        await websocket_manager.broadcast_status("listening")
        # âœ… Run AI processing asynchronously
        print("ğŸš€ Creating AI Processing Task...")
        task = asyncio.create_task(process_ai_response(buffered_transcript.strip()))
        await asyncio.sleep(0)
        print(f"âœ… AI Processing Task Created: {task}")
        processing_state = False
    except Exception as e:
        print(f"âŒ Error in processing_transcription: {e}")


async def notify_silence():
    """Enter Idle Mode after 5 seconds of silence."""
    global running, idle

    await asyncio.sleep(7)  # âœ… If silence persists, enter idle mode

    if running:
        print("ğŸ”• No speech detected. Entering Idle Mode...")
        idle = True  # âœ… Set Idle Mode ON
        await websocket_manager.broadcast_status("idle")
        await stop_transcription()  # âœ… Stop Google Streaming


async def start_transcription():
    """Start transcription only if not already running."""
    global running, idle, client

    if running or processing_state:
        return
    running = True
    idle = False  # âœ… Exit Idle Mode

    print("ğŸ¤ Starting fresh speech recognition...")
    
    # âœ… Always create a fresh SpeechClient on start
    client = speech.SpeechClient()

    await websocket_manager.broadcast_status("listening")
    
    audio_interface = pyaudio.PyAudio()
    stream = audio_interface.open(
        format=pyaudio.paInt16,
        channels=1,
        rate=RATE,
        input=True,
        frames_per_buffer=CHUNK,
        stream_callback=audio_callback,
    )
    if USE_WHISPER:
        asyncio.create_task(transcribe_whisper())
    else:
        asyncio.create_task(transcribe_google())

        
    print("âœ… Fresh speech recognition started")





async def stop_transcription():
    """Stop the audio stream and recognition completely."""
    global running, audio_queue

    if not running:
        return
    running = False  # âœ… Stop further streaming

    await websocket_manager.broadcast_status("stopped")
    
    # âœ… Fully clear the audio queue to remove old data
    with audio_queue.mutex:
        audio_queue.queue.clear()

    # âœ… Properly end the generator
    audio_queue.put(None)  # ğŸ”¥ Sends a signal to stop `audio_generator()`

    # âœ… Reset Google SpeechClient (for safety)
    global client
    client = None

    print("ğŸ›‘ Speech recognition fully stopped.")

